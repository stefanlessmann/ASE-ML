{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1s_gTSh_T_ww2K3QOwS9k70iRQapNLy4b","timestamp":1680093023199}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/stefanlessmann/ASE-ML/blob/main/Day-1-Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# Wecome to our Section 12 Deep Learning Practice Session\n","Today, we put everything together studied thus far and develop a fully-fledged pipeline for training, testing, and model selecting a neural network classifier to predict the risk of credit default. \n","\n","The outline of today's session is as follows:\n","- Load real-world credit data from GitHub\n","- Eyeballing the data using the `Pandas` library\n","- Perform basic data preprocessing\n","- Partition the data into training a testing data\n","- Train and assess a neural network classifier. For assessment, compute the classification accuracy of your trained network.\n","- Optional task for expert: Tuning the architecture of the neural network\n","\n","Much work to do, so let's go!\n"],"metadata":{"id":"grvYBSqLwztN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"h6vhKogUqLQY"},"outputs":[],"source":["# Standard libraries for data data handling and plotting\n","import numpy as np \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import pandas as pd\n"]},{"cell_type":"markdown","source":["## Load real-world credit data from GitHub\n","The data set we use for this session is available at the following URL: <br>\n","https://raw.githubusercontent.com/stefanlessmann/ASE-ML/master/hmeq.csv\n","\n","Create a variable named `data_url` in which you store this URL. Next, use the method `read_csv()` from the `Pandas` library to download the data right from the web and store it in a `DataFrame`. "],"metadata":{"id":"sbbga7IizBfT"}},{"cell_type":"code","source":["# Enter code to download the demo data set from the web \n"],"metadata":{"id":"d16WAzKPuCFf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Introducing the HMEQ data set (you can skip this section)\n","Our data set, called the  \"Home Equity\" or, in brief, HMEQ data set, is provided by www.creditriskanalytics.net. It comprises  information about a set of borrowers, which are categorized along demographic variables and variables concerning their business relationship with the lender. A binary target variable called 'BAD' is  provided and indicates whether a borrower has repaid her/his debt. You can think of the data as a standard use case of binary classification.\n","\n","You obtain the data, together with other interesting finance data sets, directly from www.creditriskanalytics.net. The website also provides a brief description of the data set. Specifically, the data set consists of 5,960 observations and 13 features including the target variable. The variables are defined as follows:\n","\n","- BAD: the target variable, 1=default; 0=non-default \n","- LOAN: amount of the loan request\n","- MORTDUE: amount due on an existing mortgage\n","- VALUE: value of current property\n","- REASON: DebtCon=debt consolidation; HomeImp=home improvement\n","- JOB: occupational categories\n","- YOJ: years at present job\n","- DEROG: number of major derogatory reports\n","- DELINQ: number of delinquent credit lines\n","- CLAGE: age of oldest credit line in months\n","- NINQ: number of recent credit inquiries\n","- CLNO: number of credit lines\n","- DEBTINC: debt-to-income ratio\n","\n","As you can see, the features aim at describing the financial situation of a borrower. We will keep using the data set for many modeling tasks in this demo notebook and future demo notebook. So it makes sense to familiarize yourself with the above features. Make sure you understand what type of information they provide and what this information might reveal about the risk of defaulting.  "],"metadata":{"id":"rqvOwIik6WIC"}},{"cell_type":"markdown","source":["## Eyeballing data using the Pandas library\n","In this part, we briefly demonstrate some practices for obtaining a first intuition about a data set. Let's first identify some standard methods that the `Pandas` library provides for this purpose. This is not meant to give you a fully-comprehensive list. With that disclaimer, however, it is fair to say that you will apply all of the following methods to a new data frame.  \n","- `info()` \n","- `describe()`\n","- `head()` / `tail()`\n","\n","Try them out and inspect the output. Briefly summarize your findings."],"metadata":{"id":"9kBNd8886D_o"}},{"cell_type":"code","source":["# Standard methods to exploring a DataFrame \n"],"metadata":{"id":"HXKMAQNV_y1Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creating outputs in the form of lists and tables is fine and can provide useful insights. However, you probably know the saying \"*a picture says more than a thousand words*\". Suitable graphics are often more insightful than (big) tables and can convey information in a more accessible way. Fortunately, `Pandas` offers many useful features to visualize the data within a DataFrame.\n","\n","Try  accomplish the following tasks. Web search using competendly selected search phrases will easily reveal the `Pandas` methods you need and how these are invoked. \n","- Create a *histogram* for the the dependent (aka target) variable **BAD**\n","- Create a density plot to depict the distribution of the variable **LOAN**\n","- It is more common to visualize the distribution of numerical variables using *boxplots*. Considering again the variable, **LOAN** and depict its distribution using a *boxplot*\n","- Create one last boxplot of the variable **LOAN**. This time, depict the distribution of loan amounts seperately for good and bad borrowers. To that end, use the target variable **BAD** and use it to add a grouping to your boxplot."],"metadata":{"id":"HXqdAZMZAKol"}},{"cell_type":"code","source":["# Create a histogram for the the dependent (aka target) variable BAD"],"metadata":{"id":"hbaaz89TBCv7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a density plot to depict the distribution of the variable LOAN"],"metadata":{"id":"1T6u04_kBXS2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a box plot to depict the distribution of the variable LOAN"],"metadata":{"id":"2YTuJy_wBXVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a box plot of the variable LOAN while grouping by the target"],"metadata":{"id":"trsGDeANBXXf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Perform basic data preprocessing\n","Data preparation is a topic of great importance. We could easily devote an entire lecture to this topic. Below, we sketch only a tiny little bit of the tasks typically performed in the scope of data preparation. \n","\n","In fact, we could not proceed with the data in its present form. For example, going back to the tabular preview of our data, you might notice that several variables show some missing values (denoted as NaN for *not a number* in the preview*). Leaving those missing values w/o treatment would break our neural network.\n","\n","Similarly, the data preview highlights that different variables show substantialyl different value ranges. Simply compare the values (or descriptive statistics also shown above) of the variables **LOAN** to those of **VALUE** to see this. For example, the maximal value of the variable **LOAN** is 89.900,00 whreas the largest value for the variable **VALUE** is 855.909,00 so almost ten times bigger. Differences in the value range of variables are normal but, unless suitably treated in data prepration, will also harm our neural network. \n","\n","Last, the data set includes some categorical variables. These would also need special treatment, which we do not have the time to detail. Thus, we simply delete them.\n","\n","In sum, we need to accomplish the following tasks:\n","- Deleting the categorical variables **REASON** and **JOB**\n","- Replacing all missing values in the data using *mean imputation*\n","- Transforming the value range of all variables such that values are scaled between zero and one.\n","\n","Let's go..."],"metadata":{"id":"9qRx_48h-Ydc"}},{"cell_type":"code","source":["# Delete selected columns\n","df = df.drop(columns=[\"REASON\", \"JOB\"], axis=1)\n","df.info() # verify the columns are no longer part of the DataFrame"],"metadata":{"id":"FbWoD2YsBXZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace missing values in a column with the mean of that column \n","df = df.fillna(value=df.mean())\n","df"],"metadata":{"id":"ZWOTgAr7H2xK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scale value ranges of variables to zero and one\n","# REMARK: it is important we perform this operation only on the dependent \n","#         variables and not on the dependent variable. Our target variable\n","#         remains unchanged. \n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler(feature_range=(0,1))\n","\n","# Remove the target variable so that it remains unchanged\n","y = df.pop(\"BAD\")\n","\n","# Scale the remaining columns\n","# We also create a new DataFrame to store the transformed data. \n","# This is because our scaler does not alter the original data but\n","# first creates a copy of the data and then transforms this copy.  \n","df_ready = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n","\n","# Verify our transformed data shows the same value ranges for all columns\n","df_ready.describe()  # We reuse the method .describe(), which computes min/max values for each column"],"metadata":{"id":"emd4Y6srJdIE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Partition the data into training a testing data\n","Data partitioning is standard practice to ensure we use disjoint data for training and evaluating a machine learning model. \n","\n","the `sklearn` library provides many methods to partition data for standard machine learning workflows. Arguably, the easiest approach is to use the method `train_test_split()`. Check out its documentation and partition our *prepared* data. Let's say we use 30% of the data for testing and the rest for training."],"metadata":{"id":"hxYbZz2c-eIB"}},{"cell_type":"code","source":[],"metadata":{"id":"HuGa1nKXLUBx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train and assess a neural network classifier\n","Finally, we come to the point where neural networks come in. In fact, the following exercise of training and assessing a classifier will look pretty much the same with any learning algorithm. That is one of the nice features of the `sklearn` library. It offers a consistent interface to many different learning algorithms. This way, we can easily switch from one learning algorithm to another if we like. For now, however, we focus on neural networks. \n","\n","Here is the chain to tasks we need to perform\n","- Import relevant libraries to train a neural network classifier\n","- Train our NN classifier using the training data\n","- Compute NN predictions for the test data\n","- Compute a performance indicator over the test set predictions. \n"],"metadata":{"id":"Q8AyLv82Okw1"}},{"cell_type":"code","source":["# Import relevant libraries to train a neural network classifier\n","\n","\n","# Train our NN classifier using the training data\n","\n","\n","# Compute NN predictions for the test data\n","\n","\n","# Compute a performance indicator over the test set predictions. \n","\n"],"metadata":{"id":"idSazLynPXfd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Optional task for expert: Tuning the architecture of the neural network\n","Try to improve your NN classifier by systematically evaluating alternative network architectures. For example, you can consider networks with different numbers of layers and different numbers of nodes in those layers. \n","\n","In fact, there are many other configuration options (called meta-parameters) you could consider, as we discussed in the lecture. The point of this task is not to find the best NN classifier for our specific data set. Rather, the point is to familiarize you with relevant libraries and methods to carry out such tuning tasks using Python. In the lecture, we introduced the concept of *grid search*. Time to run a web search for, e.g., \"sklearn grid search\" and see what it produces ;) "],"metadata":{"id":"SODevpVI-hS-"}},{"cell_type":"code","source":["# Trying to improve our NN by grid searching meta-parameter options"],"metadata":{"id":"uM5EjufEST6r"},"execution_count":null,"outputs":[]}]}